{
  "the-competence-trap-why-ai-makes-us-feel-smarter-while-making-us-less-capable": {
    "title": "The Competence Trap: Why AI Makes Us Feel Smarter While Making Us Less\n      Capable",
    "excerpt": "I've been working with LLMs for a while now, and I'm not here to bash AI\n      or ignore how useful it can be. But I've noticed some troubling patterns\n      that I think we need to talk about.",
    "content": "\n    <h1 id=\"The Competence Trap: Why AI Makes Us Feel Smarter While Making Us Less Capable\">\n      The Competence Trap: Why AI Makes Us Feel Smarter While Making Us Less\n      Capable\n    </h1>\n    <p></p>\n    <br>\n    <p>\n      I've been working with LLMs for a while now, and I'm not here to bash AI\n      or ignore how useful it can be. But I've noticed some troubling patterns\n      that I think we need to talk about.\n    </p>\n    <h2 id=\"Information Always Gets Lost in Translation\">\n      Information Always Gets Lost in Translation\n    </h2>\n    <p>\n      Here's something I've been thinking about: when information passes from\n      person to person (whether it's through 10 people or 100), it gets\n      distorted. By the time it reaches the end of the chain, it barely\n      resembles what the original person intended to communicate.<br>\n      The same thing happens when we communicate with AI models. No AI system\n      can fully capture what we mean, just like humans can't perfectly\n      understand each other either. There's always something lost in\n      translation.\n    </p>\n    <h2 id=\"The &quot;Magic Oracle&quot; Problem\">The \"Magic Oracle\" Problem</h2>\n    <p>\n      I see this information loss playing out in a particularly dangerous way\n      with non-technical users. They treat AI like some kind of magic oracle\n      that suddenly gives them superpowers to build things they never could\n      before.<br>\n      Don't get me wrong though. They can absolutely create functional web apps\n      and desktop applications now. I watched a YouTube video recently where\n      someone used AI to build a macOS app for analyzing iPhone health data. It\n      worked but it was basically just displaying information in charts, not\n      actually analyzing anything meaningful. The creator didn't have the\n      statistical or mathematical background to turn that data into real\n      insights.\n    </p>\n    <h3 id=\"The App Store Gold Rush Mentality\">\n      The App Store Gold Rush Mentality\n    </h3>\n    <p>\n      What I'm seeing more and more is this pattern: someone gets an idea, uses\n      AI to generate the code, creates a flashy marketing video (probably also\n      with AI), and then floods social media with bot accounts to promote it.\n      The goal isn't to solve real problems.\n      <b>It's to chase trends and make quick money.</b><br>\n      These apps might work on the surface, but without foundational knowledge,\n      creators are walking into potential legal and reputational disasters. What\n      happens when an app crashes and loses someone's data? What if it\n      mishandles sensitive information?\n    </p>\n    <h2 id=\"Most AI Apps Won_t Make It Past Year One\">\n      Most AI Apps Won't Make It Past Year One\n    </h2>\n    <p>\n      I'm betting most of the apps being cranked out right now won't survive\n      more than a few months. But AI does serve one genuinely valuable purpose\n      for non-technical people: creating prototypes to show technical team\n      members what they have in mind, which bridge the communication gap (a\n      little bit) between technical and non-technical people.\n    </p>\n    <h2 id=\"AI Is Still Just a Tool\">AI Is Still Just a Tool</h2>\n    <p>\n      No matter how powerful AI gets, I think it'll remain what it is today: a\n      tool for handling routine tasks. We don't implement sorting algorithms\n      from scratch anymore (except in computer science classes), but we still\n      need to understand when and why to use them.<br>\n      Here's something I've noticed about how people interact with AI:\n      non-technical users tend to ask AI to \"do something\" while technical\n      people ask \"how to do something.\" That difference in approach says\n      everything about the growing knowledge gap.\n    </p>\n    <h2 id=\"We’re Still Figuring This Out\">We’re Still Figuring This Out</h2>\n    <p>\n      We don't really know what we're doing with AI yet. \"AI engineering\" isn't\n      a mature discipline. We have some emerging patterns like prompt design and\n      human-in-the-loop workflows, but these are still evolving, not established\n      best practices.\n    </p>\n    <h2 id=\"Real-World Reality Check\">Real-World Reality Check</h2>\n    <p>\n      I've been testing this with current state-of-the-art models (DeepSeek R1,\n      Claude Sonnet 4, Opus 4, OpenAI O3, and O4-mini, etc). (Good) Human\n      engineers see the bigger picture, understand how everything fits\n      together.<br>\n      The AI-generated code? It's full of problems: unnecessary complexity,\n      violation of basic principles, spaghetti architecture, and way too many\n      emojis. I bet Linus would really be pissed off if someone vibed the\n      kernel. These are fundamental issues that any computer science program\n      teaches you to avoid.<br>\n      If our best programming models still can't meet basic engineering\n      standards, we've got a long way to go. As of the time of writing this, AI\n      generating code better be under some heavy supervision.\n    </p>\n    <h2 id=\"The Productivity Paradox Is Getting Weird\">\n      The Productivity Paradox Is Getting Weird\n    </h2>\n    <p>\n      Here's where things get economically interesting (and concerning).\n      Companies are struggling to grow at expected rates, so they're hiring\n      slower. But AI is making existing employees way more productive. This\n      creates a weird imbalance.<br>\n      When I say AI makes people more productive, I don't necessarily mean\n      they're producing better work. I mean they can suddenly tackle projects in\n      completely unfamiliar areas and juggle multiple things at once. But since\n      they don't actually understand what they're building, they either ship\n      broken products or spend ages fixing problems they created without\n      realizing it.<br>\n      I'm hearing companies ask: \"Why do we need to hire someone when AI can\n      handle this?\" (Shopify?) If demand was really booming, companies would\n      hire more people AND give them all AI tools to produce even faster.<br>\n      Instead, what's happening is that AI-equipped teams are cranking out\n      products faster than companies can find customers for them. Some companies\n      (like Microsoft) REPLACE the right Windows key with Copilot Key! Growth\n      and purchasing power have slowed down, so this productivity surge isn't\n      translating into more jobs (in the post-Covid era).\n    </p>\n    <h2 id=\"We_re About to Have a Senior Developer Problem\">\n      We're About to Have a Senior Developer Problem\n    </h2>\n    <p>\n      This is the part that really worries me: if we stop hiring junior\n      developers now, who's going to be the senior developers in 10 years?<br>\n      Companies seem to be betting that AI will completely replace junior roles.\n      Maybe that's a calculated decision, or maybe they're just caught up in the\n      hype. Either way, we might be creating our own talent shortage. What's\n      next? Offshore programming tasks to Asia (like Boeing, their fly control\n      system must be so good and reliable!).<br>\n      Additionally, investment money is pouring into AI startups that use AI to\n      build their own products. So we have AI-generated code (which often isn't\n      great) potentially being used to train future AI models. That seems like a\n      recipe for degrading quality over time.\n    </p>\n    <h2 id=\"This Isn_t Like the Textile Industry\">\n      This Isn't Like the Textile Industry\n    </h2>\n    <p>\n      People love to point to the textile industry as proof that technology\n      displacement creates new opportunities. But AI is different. Instead of\n      automating specific manual tasks, AI can do cognitive work across\n      virtually every industry.<br>\n      The scale of potential displacement isn't limited to a few industries.\n      It's the entire economy. And unlike previous technological shifts where\n      workers could learn new skills and transition to different roles, AI\n      threatens something more permanent. How would the world digest this\n      unemployment? (Well, government can hide the data\n      <u><a href=\"https://www.bbc.com/news/articles/czerwl2xee4o\">Trump's pick to lead economic data agency floats ending monthly jobs\n          report</a></u>).<br>\n      But context matters. If we had a booming economy with rising consumer\n      confidence and purchasing power, businesses would hire more people (even\n      AI-enabled ones) to meet demand.\n    </p>\n    <h2 id=\"The Most Dangerous Part: Feeling Smarter Than You Are\">\n      The Most Dangerous Part: Feeling Smarter Than You Are\n    </h2>\n    <p>\n      Here's what I think is the biggest risk: AI makes people feel more\n      intelligent than they actually are. It handles the execution so smoothly\n      that users start thinking they understand what's happening. They become\n      more ambitious, work faster, work on multiple projects simultaneously and\n      mistake the AI's output for their own competence.<br>\n      <b>AI might handle 90% of the coding tasks, but it can't do 90% of a\n        developer's actual job.</b>\n      Strategic thinking, UX design, architecture decisions, debugging complex\n      systems, understanding business requirements. These are what professional\n      development is really about.<br>\n      AI only works well when combined with real expertise and critical\n      thinking. This should be obvious, but non-technical users consistently\n      miss this. They confuse AI output with genuine understanding.\n    </p>\n    <h2 id=\"My Take: Use AI When You Could Do It Without AI\">\n      My Take: Use AI When You Could Do It Without AI\n    </h2>\n    <p>\n      Here's my rule of thumb:\n      <b>use LLMs for things you could theoretically do without them</b>. Don't\n      throw good design principles and security measures out the window just\n      because AI is involved. AI assistance doesn't excuse sloppy code or poor\n      architecture.<br>\n      The illusion of competence might be the most significant risk of\n      widespread AI adoption. It encourages overconfidence while eroding the\n      deep understanding necessary for quality work.<br>\n      We need to be smarter about this. AI is a powerful tool, but it's not\n      magic, and it's definitely not a substitute for actually knowing what\n      you're doing.\n    </p>\n  ",
    "readTime": "8 min read",
    "wordCount": 1324,
    "filename": "The Competence Trap Why AI Makes Us Feel Smarter While Making Us Less Capable.html"
  },
  "building-dailyrepo-a-solo-developer's-journey-from-scraper-to-full-stack-app": {
    "title": "Building DailyRepo: A Solo Developer's Journey from Scraper to Full-Stack\n      App",
    "excerpt": "As a CS graduate whose group projects involved more project management\n      than coding, I had a problem: I wanted to build something substantial that\n      was entirely mine. Scrolling through X (formerly Twitter), I kept seeing\n      posts about AI-powered projects and monetized side hustles, but I wanted\n      concrete data about what developers were actually building, not just what\n      they were bragging about on social media.",
    "content": "\n    <h1 id=\"Building DailyRepo: A Solo Developer_s Journey from Scraper to Full-Stack App\">\n      Building DailyRepo: A Solo Developer's Journey from Scraper to Full-Stack\n      App\n    </h1>\n    <br>\n    <p>\n      As a CS graduate whose group projects involved more project management\n      than coding, I had a problem: I wanted to build something substantial that\n      was entirely mine. Scrolling through X (formerly Twitter), I kept seeing\n      posts about AI-powered projects and monetized side hustles, but I wanted\n      concrete data about what developers were actually building, not just what\n      they were bragging about on social media.\n    </p>\n    <br>\n    <p>\n      That curiosity led to\n      <a href=\"https://dailyrepo.tianpai.io\">Daily Repo</a>\n      , a full-stack application that tracks GitHub's trending repositories with\n      historical data, developer rankings, and insight analytics. What started\n      as a simple DOM scraper evolved into a TypeScript-powered system\n      processing 150+ repositories daily, complete with intelligent rate\n      limiting, conditional caching, and a custom ASCII design language.\n    </p>\n    <br>\n    <p>\n      This is the story of building my first solo full-stack project: the\n      technical challenges, architectural decisions, and surprising insights\n      discovered along the way.\n    </p>\n    <br>\n    <h2 id=\"The Problem: Data vs. Feelings\">The Problem: Data vs. Feelings</h2>\n    <br>\n    <p>\n      I was getting annoyed by the constant stream of project bragging and \"AI\n      is taking everyone's jobs\" grandiloquence flooding my feed. As someone who\n      relied heavily on feelings and assumptions about tech trends, I recognized\n      my own bias. Warren Buffett once noted that having 1,000 people tell you\n      you're right doesn't make you right. What matters is that the reasoning is\n      correct.\n    </p>\n    <br>\n    <p>\n      GitHub's trending page provided some insight, but it was missing the\n      historical context I craved. Why does a 5-year-old repository suddenly\n      surge in popularity? What makes a 10-year-old project still relevant?\n      These questions couldn't be answered by a simple trending list.\n    </p>\n    <br>\n    <p>\n      I needed to see patterns over time, understand what languages people were\n      actually using for different types of projects, and get beyond the\n      surface-level hype.\n    </p>\n    <br>\n    <h2 id=\"Technical Evolution: From Simple Scraper to Full System\">\n      Technical Evolution: From Simple Scraper to Full System\n    </h2>\n    <br>\n    <h3 id=\"Phase 1: The DOM Scraping Foundation\">\n      Phase 1: The DOM Scraping Foundation\n    </h3>\n    <br>\n    <p>\n      My technical background was basic: C, Python, HTML, CSS, JavaScript,\n      Node.js, and a bit of Express. No React experience, no TypeScript, no\n      production deployment knowledge (school projects are all 'deployed' on\n      <code class=\"code-inline\">localhost: 3000</code>). But I subscribed to the\n      philosophy that as a software engineer, it's not about the programming\n      language. If COBOL is the best tool for a project, I'll learn COBOL.\n    </p>\n    <br>\n    <p>\n      The first two weeks were spent building a DOM scraper using Cheerio and\n      Axios to fetch repository data from GitHub's trending page, since there's\n      no official API for trending repos. I set up MongoDB to store the data and\n      began the process that would become my daily nightmare: fighting GitHub's\n      rate limits.\n    </p>\n    <br>\n    <p>\n      I discovered the Star History project and learned how to fetch historical\n      star data through GitHub's API. But this meant making hundreds of API\n      requests daily, and I quickly hit the infamous 403 errors that would\n      plague my development for weeks.\n    </p>\n    <br>\n    <h3 id=\"Phase 2: The Rate Limiting Wars\">\n      Phase 2: The Rate Limiting Wars\n    </h3>\n    <br>\n    <p>\n      GitHub's API has a 5,000 hourly rate limit with personal tokens, plus\n      undocumented sub-rate limits designed to prevent abuse. Those first two\n      weeks involved constant 403 errors, timeout handling, and manually adding\n      delays between requests.\n    </p>\n    <br>\n    <p>\n      The breakthrough came from implementing intelligent error handling that\n      checks rate limit reset times rather than blindly retrying. I also added a\n      \"pre-warming\" request 20-30 minutes before the main scraping job, so by\n      the time I hit the hourly limit, I'd only need to wait 5-10 minutes\n      instead of a full hour.\n    </p>\n    <br>\n    <p>\n      <b>Result</b>: Scraping time dropped from 2+ hours to under 50 minutes for\n      150+ repositories and their complete star histories.\n    </p>\n    <br>\n    <h3 id=\"Phase 3: The JavaScript to TypeScript Migration\">\n      Phase 3: The JavaScript to TypeScript Migration\n    </h3>\n    <br>\n    <p>\n      As the backend grew beyond basic repository listings, I hit the familiar\n      problem of variables typed as <code class=\"code-inline\">any</code> leading\n      to unpredictable bugs. I was constantly using vim's\n      <code class=\"code-inline\">gd</code> command to jump into functions and\n      check return types and values.\n    </p>\n    <br>\n    <p>\n      I dedicated three days to learning TypeScript essentials and switched to\n      Bun as the runtime for native TypeScript support, eliminating build steps\n      entirely. The migration wasn't about TypeScript evangelism; it was about\n      static error checking and better developer experience.\n    </p>\n    <br>\n    <h3 id=\"Phase 4: Frontend Architecture Learning\">\n      Phase 4: Frontend Architecture Learning\n    </h3>\n    <br>\n    <p>\n      I chose React + Vite + TailwindCSS over full-stack frameworks like\n      Next.js, which felt like overkill. Despite having zero frontend\n      experience, I embraced the learning opportunity.\n    </p>\n    <br>\n    <p>\n      The biggest hurdle was mastering\n      <code class=\"code-inline\">useEffect</code> for data fetching. The hook's\n      complexity can lead to infinite loops and unnecessary renders. React's\n      documentation on \"You Might Not Need an Effect\" became essential reading\n      during this phase. It is easier to create a infinite loop.\n    </p>\n    <br>\n    <p>\n      I also moved away from Shadcn UI, which made the app look too generic.\n      Instead, I developed a custom ASCII design language that reflects the\n      developer-focused nature of the project.\n    </p>\n    <br>\n    <h2 id=\"Key Architecture Decisions\">Key Architecture Decisions</h2>\n    <br>\n    <h3 id=\"JIT Abstraction: Avoiding the Over-Engineering Trap\">\n      JIT Abstraction: Avoiding the Over-Engineering Trap\n    </h3>\n    <br>\n    <p>\n      I developed what I call \"JIT abstraction,\" borrowing from compiler\n      terminology, which means adding abstraction layers exactly when needed,\n      not before.\n    </p>\n    <br>\n    <p>The process involves:</p>\n    <ol start=\"1\" style=\"--data-list-start: 1\">\n      <li>Writing similar code patterns (2-3 similar controllers)</li>\n      <li>Identifying common behaviors and pain points</li>\n      <li>\n        Abstracting when patterns become clear and maintenance becomes difficult\n      </li>\n      <li>Refactoring before complexity becomes unmanageable</li>\n    </ol>\n    <br>\n    <p>Here's an example of controller evolution:</p>\n    <br>\n    <p><b>Before</b>: Fat controllers handling everything</p>\n    <pre class=\"fenced-code\"><code class=\"fenced-code-content\" lang=\"javascript\">app.get(<span class=\"code_string\">'/trending/:language'</span>, <span class=\"code_keyword\">async</span> (req, res) <span class=\"code_entity\">=&gt;</span> {\n  <span class=\"code_comment\">// 130+ lines of validation, DB queries, formatting, error handling...</span>\n});\n</code></pre>\n    <br>\n    <p><b>After</b>: Lean controllers with service layer</p>\n    <pre class=\"fenced-code\"><code class=\"fenced-code-content\" lang=\"typescript\"><span class=\"code_keyword\">export</span> <span class=\"code_keyword\">async</span> <span class=\"code_keyword\">function</span> <span class=\"code_function\">getTrending</span>(req<span class=\"code_entity\">:</span> Request, res<span class=\"code_entity\">:</span> Response, _next<span class=\"code_entity\">:</span> NextFunction)<span class=\"code_entity\">:</span> <span class=\"code_keyword\">Promise</span><span class=\"code_entity\">&lt;</span><span class=\"code_keyword\">void</span><span class=\"code_entity\">&gt;</span> {\n  <span class=\"code_keyword\">try</span> {\n    <span class=\"code_keyword\">const</span> params <span class=\"code_entity\">=</span> <span class=\"code_function\">parseTrendingParams</span>(req);\n    <span class=\"code_keyword\">const</span> { repoList, fromCache } <span class=\"code_entity\">=</span> <span class=\"code_keyword\">await</span> <span class=\"code_function\">fetchTrendingWithCache</span>(params.date);\n    <span class=\"code_keyword\">const</span> response <span class=\"code_entity\">=</span> <span class=\"code_function\">formatTrendingResponse</span>(repoList, params, fromCache);\n    \n    res.<span class=\"code_function\">status</span>(<span class=\"code_number\">200</span>).<span class=\"code_function\">json</span>(response);\n  } <span class=\"code_keyword\">catch</span> (error) {\n    <span class=\"code_keyword\">const</span> message <span class=\"code_entity\">=</span> error <span class=\"code_keyword\">instanceof</span> <span class=\"code_function\">Error</span> <span class=\"code_entity\">?</span> error.message <span class=\"code_entity\">:</span> <span class=\"code_string\">\"Failed to fetch trending repos\"</span>;\n    res.<span class=\"code_function\">status</span>(<span class=\"code_number\">400</span>).<span class=\"code_function\">json</span>(<span class=\"code_function\">makeError</span>(<span class=\"code_keyword\">new</span> <span class=\"code_function\">Date</span>().<span class=\"code_function\">toISOString</span>(), <span class=\"code_number\">400</span>, message));\n  }\n}\n</code></pre>\n    <br>\n    <p>\n      This approach prevented both code duplication and premature optimization,\n      a common trap in personal projects where you want to showcase architecture\n      skills.\n    </p>\n    <br>\n    <h3 id=\"Conditional Caching Strategy\">Conditional Caching Strategy</h3>\n    <br>\n    <p>\n      I implemented a <code class=\"code-inline\">withCache</code> utility that\n      combines caching logic with conditional storage:\n    </p>\n    <br>\n    <pre class=\"fenced-code\"><code class=\"fenced-code-content\" lang=\"typescript\"><span class=\"code_keyword\">export</span> <span class=\"code_keyword\">async</span> <span class=\"code_keyword\">function</span> <span class=\"code_function\">withCache</span><span class=\"code_entity\">&lt;</span><span class=\"code_constant\">T</span><span class=\"code_entity\">&gt;</span>(\n  cacheKey<span class=\"code_entity\">:</span> <span class=\"code_keyword\">string</span>,\n  <span class=\"code_function\">fetchFn</span><span class=\"code_entity\">:</span> () <span class=\"code_entity\">=&gt;</span> <span class=\"code_keyword\">Promise</span><span class=\"code_entity\">&lt;</span><span class=\"code_constant\">T</span><span class=\"code_entity\">&gt;</span>,\n  ttl<span class=\"code_entity\">:</span> <span class=\"code_keyword\">number</span>,\n  shouldCache<span class=\"code_entity\">?:</span> (data<span class=\"code_entity\">:</span> <span class=\"code_constant\">T</span>) <span class=\"code_entity\">=&gt;</span> <span class=\"code_keyword\">boolean</span>,\n)<span class=\"code_entity\">:</span> <span class=\"code_keyword\">Promise</span><span class=\"code_entity\">&lt;</span>{ data<span class=\"code_entity\">:</span> <span class=\"code_constant\">T</span>; fromCache<span class=\"code_entity\">:</span> <span class=\"code_keyword\">boolean</span> }<span class=\"code_entity\">&gt;</span> {\n  <span class=\"code_keyword\">const</span> cached <span class=\"code_entity\">=</span> <span class=\"code_function\">getCache</span>(cacheKey) <span class=\"code_keyword\">as</span> <span class=\"code_constant\">T</span>;\n  <span class=\"code_keyword\">if</span> (cached) {\n    <span class=\"code_keyword\">return</span> { data<span class=\"code_entity\">:</span> cached, fromCache<span class=\"code_entity\">:</span> <span class=\"code_number\">true</span> };\n  }\n\n  <span class=\"code_keyword\">const</span> data <span class=\"code_entity\">=</span> <span class=\"code_keyword\">await</span> <span class=\"code_function\">fetchFn</span>();\n\n  <span class=\"code_comment\">// Only cache if shouldCache function returns true</span>\n  <span class=\"code_keyword\">if</span> (<span class=\"code_entity\">!</span>shouldCache <span class=\"code_entity\">||</span> <span class=\"code_function\">shouldCache</span>(data)) {\n    <span class=\"code_function\">setCache</span>(cacheKey, data, ttl);\n  }\n\n  <span class=\"code_keyword\">return</span> { data, fromCache<span class=\"code_entity\">:</span> <span class=\"code_number\">false</span> };\n}\n</code></pre>\n    <br>\n    <p>\n      This pattern allowed me to cache search results only when data was found,\n      avoiding expensive cache misses for failed searches. Working within\n      MongoDB's free tier constraints, every optimization mattered.\n    </p>\n    <br>\n    <h3 id=\"Batched Database Operations\">Batched Database Operations</h3>\n    <br>\n    <p>\n      As the system scaled from 15 daily repositories to 150+, I moved from\n      individual database calls to batched operations. This reduced database\n      calls significantly while working within free tier limitations, though the\n      performance improvement was less dramatic than the rate limiting\n      optimizations.\n    </p>\n    <br>\n    <h2 id=\"Performance &amp; Scaling Lessons\">Performance &amp; Scaling Lessons</h2>\n    <br>\n    <h3 id=\"Rate Limiting Optimization\">Rate Limiting Optimization</h3>\n    <br>\n    <p>\n      The biggest performance breakthrough came from understanding GitHub's\n      undocumented sub-rate limits. Rather than implementing a complex event\n      loop system (which would still hit the 5,000/hour ceiling), I focused on\n      minimizing delays and handling 403 errors intelligently.\n    </p>\n    <br>\n    <p>\n      The system now makes a single request to GitHub's rate limit endpoint when\n      it encounters a 403, calculates the exact wait time, and pauses scraping\n      until the limit resets. This eliminated the cascading 403 errors that\n      previously extended scraping times.\n    </p>\n    <br>\n    <h3 id=\"Deployment Platform Juggling\">Deployment Platform Juggling</h3>\n    <br>\n    <p>\n      Working with free tiers meant constant deployment platform switching:\n      Vercel for frontend, Railway for backend (after Render's custom domain\n      issues), and GitHub Actions for scheduled scraping. Each platform had its\n      quirks: Bun deployment issues on Railway, MongoDB connection problems in\n      GitHub Actions, CORS configuration headaches.\n    </p>\n    <br>\n    <p>\n      These constraints taught me to design systems that can adapt to different\n      deployment environments rather than being tightly coupled to specific\n      platforms.\n    </p>\n    <br>\n    <h2 id=\"Surprising Data Insights\">Surprising Data Insights</h2>\n    <br>\n    <p>\n      The data revealed patterns that contradicted my assumptions about modern\n      development:\n    </p>\n    <br>\n    <p>\n      <b>Language Distribution</b>: C and C++ still dominate trending\n      repositories (32% and 19% respectively), followed by Java (17%). This was\n      genuinely surprising in 2025.\n    </p>\n    <br>\n    <p><b>Language-Specific Patterns</b>:</p>\n    <ul>\n      <li>\n        <b>Go</b>: Primarily backend infrastructure (Kubernetes, microservices,\n        databases)\n      </li>\n      <li>\n        <b>TypeScript</b>: Heavily React-focused rather than general web\n        development\n      </li>\n      <li>\n        <b>Java</b>: Still relevant for Android, Kafka, and surprisingly,\n        algorithm/interview prep\n      </li>\n      <li><b>Python</b>: Concentrated in AI/ML research as expected</li>\n      <li>\n        <b>Rust</b>: Most evenly distributed across topics, unique in blockchain\n        representation\n      </li>\n    </ul>\n    <br>\n    <p>\n      <b>NOTE</b>: However, I suspect this language dominance reflects\n      structural differences rather than pure popularity. C/C++ projects often\n      need to rebuild foundational tools that higher-level languages import as\n      packages. Java's verbosity might mean even simple concepts require\n      substantial codebases that attract more attention.\n    </p>\n    <br>\n    <p>\n      <b>Trending Anomalies</b>: Developers often trend without their\n      repositories trending, suggesting GitHub's algorithms work differently for\n      people versus projects, possibly based on contributions to established\n      projects rather than new repository creation.\n    </p>\n    <br>\n    <p>\n      <b>Infrastructure Details</b>: Even seemingly simple features required\n      hunting down community-maintained resources, like finding a comprehensive\n      GitHub language-to-color mapping that someone had reverse-engineered and\n      shared.\n    </p>\n    <br>\n    <h2 id=\"What I_d Do Differently\">What I'd Do Differently</h2>\n    <br>\n    <h3 id=\"Start with a Monorepo\">Start with a Monorepo</h3>\n    <br>\n    <p>\n      Keeping frontend and backend in separate repositories created deployment\n      coordination headaches and duplicated TypeScript interfaces. A monorepo\n      would have simplified shared types, development environment setup, and\n      coordinated deployments.\n    </p>\n    <br>\n    <h3 id=\"Design-First Approach\">Design-First Approach</h3>\n    <br>\n    <p>\n      Having gained experience, I would prioritize the user interface design\n      earlier. The current ASCII design language evolved organically but would\n      benefit from more systematic planning. Understanding how to present the\n      data effectively should inform the backend API design.\n    </p>\n    <br>\n    <h3 id=\"Unified Deployment Architecture\">\n      Unified Deployment Architecture\n    </h3>\n    <br>\n    <p>\n      Running the scraper, server, and frontend on the same platform would\n      reduce complexity and deployment coordination issues. The current split\n      across GitHub Actions, Railway, and Vercel works but adds unnecessary\n      operational overhead.\n    </p>\n    <br>\n    <h2 id=\"The Meta-Learning\">The Meta-Learning</h2>\n    <br>\n    <p>\n      Building DailyRepo taught me that good software development isn't just\n      about code. It's about understanding problems, designing solutions, and\n      iterating based on real-world constraints.\n    </p>\n    <br>\n    <p>\n      The temptation with full-stack development is to master every layer\n      perfectly before moving forward. Instead, I learned to embrace\n      \"progressive competence,\" building working solutions while gradually\n      deepening understanding of the underlying technologies.\n    </p>\n    <br>\n    <p>\n      <b>Most importantly</b>: The ability to research and learn efficiently\n      often matters more than existing knowledge. Every constraint from GitHub's\n      rate limits to free tier database restrictions became an opportunity to\n      develop better engineering judgment.\n    </p>\n    <br>\n    <p>\n      The data collection continues automatically, the insights keep revealing\n      new questions, and the codebase remains a living testament to the\n      iterative nature of building something substantial alone. Sometimes the\n      best way to understand what developers are actually working on is to\n      become one of them.\n    </p>\n    <hr>\n    <br>\n    <p>\n      <i>View the project at\n        <a href=\"https://dailyrepo.tianpai.io\">dailyrepo.tianpai.io</a> | Source\n        code:\n        <a href=\"https://github.com/tianpai/dailyRepo\">github.com/tianpai/dailyRepo</a></i>\n    </p>\n  ",
    "readTime": "10 min read",
    "wordCount": 1672,
    "filename": "Building DailyRepo A Solo Developer's Journey from Scraper to Full-Stack App.html"
  }
}