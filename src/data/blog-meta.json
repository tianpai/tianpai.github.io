{
  "the-competence-trap-why-ai-makes-us-feel-smarter-while-making-us-less-capable": {
    "title": "The Competence Trap: Why AI Makes Us Feel Smarter While Making Us Less\n      Capable",
    "excerpt": "I've been working with LLMs for a while now, and I'm not here to bash AI\n      or ignore how useful it can be. But I've noticed some troubling patterns\n      that I think we need to talk about.",
    "content": "\n    <h1 id=\"The Competence Trap: Why AI Makes Us Feel Smarter While Making Us Less Capable\">\n      The Competence Trap: Why AI Makes Us Feel Smarter While Making Us Less\n      Capable\n    </h1>\n    <p></p>\n    <br>\n    <p>\n      I've been working with LLMs for a while now, and I'm not here to bash AI\n      or ignore how useful it can be. But I've noticed some troubling patterns\n      that I think we need to talk about.\n    </p>\n    <h2 id=\"Information Always Gets Lost in Translation\">\n      Information Always Gets Lost in Translation\n    </h2>\n    <p>\n      Here's something I've been thinking about: when information passes from\n      person to person (whether it's through 10 people or 100), it gets\n      distorted. By the time it reaches the end of the chain, it barely\n      resembles what the original person intended to communicate.<br>\n      The same thing happens when we communicate with AI models. No AI system\n      can fully capture what we mean, just like humans can't perfectly\n      understand each other either. There's always something lost in\n      translation.\n    </p>\n    <h2 id=\"The &quot;Magic Oracle&quot; Problem\">The \"Magic Oracle\" Problem</h2>\n    <p>\n      I see this information loss playing out in a particularly dangerous way\n      with non-technical users. They treat AI like some kind of magic oracle\n      that suddenly gives them superpowers to build things they never could\n      before.<br>\n      Don't get me wrong though. They can absolutely create functional web apps\n      and desktop applications now. I watched a YouTube video recently where\n      someone used AI to build a macOS app for analyzing iPhone health data. It\n      worked but it was basically just displaying information in charts, not\n      actually analyzing anything meaningful. The creator didn't have the\n      statistical or mathematical background to turn that data into real\n      insights.\n    </p>\n    <h3 id=\"The App Store Gold Rush Mentality\">\n      The App Store Gold Rush Mentality\n    </h3>\n    <p>\n      What I'm seeing more and more is this pattern: someone gets an idea, uses\n      AI to generate the code, creates a flashy marketing video (probably also\n      with AI), and then floods social media with bot accounts to promote it.\n      The goal isn't to solve real problems.\n      <b>It's to chase trends and make quick money.</b><br>\n      These apps might work on the surface, but without foundational knowledge,\n      creators are walking into potential legal and reputational disasters. What\n      happens when an app crashes and loses someone's data? What if it\n      mishandles sensitive information?\n    </p>\n    <h2 id=\"Most AI Apps Won_t Make It Past Year One\">\n      Most AI Apps Won't Make It Past Year One\n    </h2>\n    <p>\n      I'm betting most of the apps being cranked out right now won't survive\n      more than a few months. But AI does serve one genuinely valuable purpose\n      for non-technical people: creating prototypes to show technical team\n      members what they have in mind, which bridge the communication gap (a\n      little bit) between technical and non-technical people.\n    </p>\n    <h2 id=\"AI Is Still Just a Tool\">AI Is Still Just a Tool</h2>\n    <p>\n      No matter how powerful AI gets, I think it'll remain what it is today: a\n      tool for handling routine tasks. We don't implement sorting algorithms\n      from scratch anymore (except in computer science classes), but we still\n      need to understand when and why to use them.<br>\n      Here's something I've noticed about how people interact with AI:\n      non-technical users tend to ask AI to \"do something\" while technical\n      people ask \"how to do something.\" That difference in approach says\n      everything about the growing knowledge gap.\n    </p>\n    <h2 id=\"We’re Still Figuring This Out\">We’re Still Figuring This Out</h2>\n    <p>\n      We don't really know what we're doing with AI yet. \"AI engineering\" isn't\n      a mature discipline. We have some emerging patterns like prompt design and\n      human-in-the-loop workflows, but these are still evolving, not established\n      best practices.\n    </p>\n    <h2 id=\"Real-World Reality Check\">Real-World Reality Check</h2>\n    <p>\n      I've been testing this with current state-of-the-art models (DeepSeek R1,\n      Claude Sonnet 4, Opus 4, OpenAI O3, and O4-mini, etc). (Good) Human\n      engineers see the bigger picture, understand how everything fits\n      together.<br>\n      The AI-generated code? It's full of problems: unnecessary complexity,\n      violation of basic principles, spaghetti architecture, and way too many\n      emojis. I bet Linus would really be pissed off if someone vibed the\n      kernel. These are fundamental issues that any computer science program\n      teaches you to avoid.<br>\n      If our best programming models still can't meet basic engineering\n      standards, we've got a long way to go. As of the time of writing this, AI\n      generating code better be under some heavy supervision.\n    </p>\n    <h2 id=\"The Productivity Paradox Is Getting Weird\">\n      The Productivity Paradox Is Getting Weird\n    </h2>\n    <p>\n      Here's where things get economically interesting (and concerning).\n      Companies are struggling to grow at expected rates, so they're hiring\n      slower. But AI is making existing employees way more productive. This\n      creates a weird imbalance.<br>\n      When I say AI makes people more productive, I don't necessarily mean\n      they're producing better work. I mean they can suddenly tackle projects in\n      completely unfamiliar areas and juggle multiple things at once. But since\n      they don't actually understand what they're building, they either ship\n      broken products or spend ages fixing problems they created without\n      realizing it.<br>\n      I'm hearing companies ask: \"Why do we need to hire someone when AI can\n      handle this?\" (Shopify?) If demand was really booming, companies would\n      hire more people AND give them all AI tools to produce even faster.<br>\n      Instead, what's happening is that AI-equipped teams are cranking out\n      products faster than companies can find customers for them. Some companies\n      (like Microsoft) REPLACE the right Windows key with Copilot Key! Growth\n      and purchasing power have slowed down, so this productivity surge isn't\n      translating into more jobs (in the post-Covid era).\n    </p>\n    <h2 id=\"We_re About to Have a Senior Developer Problem\">\n      We're About to Have a Senior Developer Problem\n    </h2>\n    <p>\n      This is the part that really worries me: if we stop hiring junior\n      developers now, who's going to be the senior developers in 10 years?<br>\n      Companies seem to be betting that AI will completely replace junior roles.\n      Maybe that's a calculated decision, or maybe they're just caught up in the\n      hype. Either way, we might be creating our own talent shortage. What's\n      next? Offshore programming tasks to Asia (like Boeing, their fly control\n      system must be so good and reliable!).<br>\n      Additionally, investment money is pouring into AI startups that use AI to\n      build their own products. So we have AI-generated code (which often isn't\n      great) potentially being used to train future AI models. That seems like a\n      recipe for degrading quality over time.\n    </p>\n    <h2 id=\"This Isn_t Like the Textile Industry\">\n      This Isn't Like the Textile Industry\n    </h2>\n    <p>\n      People love to point to the textile industry as proof that technology\n      displacement creates new opportunities. But AI is different. Instead of\n      automating specific manual tasks, AI can do cognitive work across\n      virtually every industry.<br>\n      The scale of potential displacement isn't limited to a few industries.\n      It's the entire economy. And unlike previous technological shifts where\n      workers could learn new skills and transition to different roles, AI\n      threatens something more permanent. How would the world digest this\n      unemployment? (Well, government can hide the data\n      <u><a href=\"https://www.bbc.com/news/articles/czerwl2xee4o\">Trump's pick to lead economic data agency floats ending monthly jobs\n          report</a></u>).<br>\n      But context matters. If we had a booming economy with rising consumer\n      confidence and purchasing power, businesses would hire more people (even\n      AI-enabled ones) to meet demand.\n    </p>\n    <h2 id=\"The Most Dangerous Part: Feeling Smarter Than You Are\">\n      The Most Dangerous Part: Feeling Smarter Than You Are\n    </h2>\n    <p>\n      Here's what I think is the biggest risk: AI makes people feel more\n      intelligent than they actually are. It handles the execution so smoothly\n      that users start thinking they understand what's happening. They become\n      more ambitious, work faster, work on multiple projects simultaneously and\n      mistake the AI's output for their own competence.<br>\n      <b>AI might handle 90% of the coding tasks, but it can't do 90% of a\n        developer's actual job.</b>\n      Strategic thinking, UX design, architecture decisions, debugging complex\n      systems, understanding business requirements. These are what professional\n      development is really about.<br>\n      AI only works well when combined with real expertise and critical\n      thinking. This should be obvious, but non-technical users consistently\n      miss this. They confuse AI output with genuine understanding.\n    </p>\n    <h2 id=\"My Take: Use AI When You Could Do It Without AI\">\n      My Take: Use AI When You Could Do It Without AI\n    </h2>\n    <p>\n      Here's my rule of thumb:\n      <b>use LLMs for things you could theoretically do without them</b>. Don't\n      throw good design principles and security measures out the window just\n      because AI is involved. AI assistance doesn't excuse sloppy code or poor\n      architecture.<br>\n      The illusion of competence might be the most significant risk of\n      widespread AI adoption. It encourages overconfidence while eroding the\n      deep understanding necessary for quality work.<br>\n      We need to be smarter about this. AI is a powerful tool, but it's not\n      magic, and it's definitely not a substitute for actually knowing what\n      you're doing.\n    </p>\n  ",
    "readTime": "8 min read",
    "wordCount": 1324,
    "filename": "The Competence Trap Why AI Makes Us Feel Smarter While Making Us Less Capable.html"
  }
}