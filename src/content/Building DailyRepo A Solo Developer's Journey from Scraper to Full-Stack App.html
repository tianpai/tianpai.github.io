<body>
  <div class="document-wrapper">
    <h1
      id="Building DailyRepo: A Solo Developer_s Journey from Scraper to Full-Stack App"
    >
      Building DailyRepo: A Solo Developer's Journey from Scraper to Full-Stack
      App
    </h1>
    <br />
    <p>
      As a CS graduate whose group projects involved more project management
      than coding, I had a problem: I wanted to build something substantial that
      was entirely mine. Scrolling through X (formerly Twitter), I kept seeing
      posts about AI-powered projects and monetized side hustles, but I wanted
      concrete data about what developers were actually building, not just what
      they were bragging about on social media.
    </p>
    <br />
    <p>
      That curiosity led to
      <a href="https://dailyrepo.tianpai.io">Daily Repo</a>
      , a full-stack application that tracks GitHub's trending repositories with
      historical data, developer rankings, and insight analytics. What started
      as a simple DOM scraper evolved into a TypeScript-powered system
      processing 150+ repositories daily, complete with intelligent rate
      limiting, conditional caching, and a custom ASCII design language.
    </p>
    <br />
    <p>
      This is the story of building my first solo full-stack project: the
      technical challenges, architectural decisions, and surprising insights
      discovered along the way.
    </p>
    <br />
    <h2 id="The Problem: Data vs. Feelings">The Problem: Data vs. Feelings</h2>
    <br />
    <p>
      I was getting annoyed by the constant stream of project bragging and "AI
      is taking everyone's jobs" grandiloquence flooding my feed. As someone who
      relied heavily on feelings and assumptions about tech trends, I recognized
      my own bias. Warren Buffett once noted that having 1,000 people tell you
      you're right doesn't make you right. What matters is that the reasoning is
      correct.
    </p>
    <br />
    <p>
      GitHub's trending page provided some insight, but it was missing the
      historical context I craved. Why does a 5-year-old repository suddenly
      surge in popularity? What makes a 10-year-old project still relevant?
      These questions couldn't be answered by a simple trending list.
    </p>
    <br />
    <p>
      I needed to see patterns over time, understand what languages people were
      actually using for different types of projects, and get beyond the
      surface-level hype.
    </p>
    <br />
    <h2 id="Technical Evolution: From Simple Scraper to Full System">
      Technical Evolution: From Simple Scraper to Full System
    </h2>
    <br />
    <h3 id="Phase 1: The DOM Scraping Foundation">
      Phase 1: The DOM Scraping Foundation
    </h3>
    <br />
    <p>
      My technical background was basic: C, Python, HTML, CSS, JavaScript,
      Node.js, and a bit of Express. No React experience, no TypeScript, no
      production deployment knowledge (school projects are all 'deployed' on
      <code class="code-inline">localhost: 3000</code>). But I subscribed to the
      philosophy that as a software engineer, it's not about the programming
      language. If COBOL is the best tool for a project, I'll learn COBOL.
    </p>
    <br />
    <p>
      The first two weeks were spent building a DOM scraper using Cheerio and
      Axios to fetch repository data from GitHub's trending page, since there's
      no official API for trending repos. I set up MongoDB to store the data and
      began the process that would become my daily nightmare: fighting GitHub's
      rate limits.
    </p>
    <br />
    <p>
      I discovered the Star History project and learned how to fetch historical
      star data through GitHub's API. But this meant making hundreds of API
      requests daily, and I quickly hit the infamous 403 errors that would
      plague my development for weeks.
    </p>
    <br />
    <h3 id="Phase 2: The Rate Limiting Wars">
      Phase 2: The Rate Limiting Wars
    </h3>
    <br />
    <p>
      GitHub's API has a 5,000 hourly rate limit with personal tokens, plus
      undocumented sub-rate limits designed to prevent abuse. Those first two
      weeks involved constant 403 errors, timeout handling, and manually adding
      delays between requests.
    </p>
    <br />
    <p>
      The breakthrough came from implementing intelligent error handling that
      checks rate limit reset times rather than blindly retrying. I also added a
      "pre-warming" request 20-30 minutes before the main scraping job, so by
      the time I hit the hourly limit, I'd only need to wait 5-10 minutes
      instead of a full hour.
    </p>
    <br />
    <p>
      <b>Result</b>: Scraping time dropped from 2+ hours to under 50 minutes for
      150+ repositories and their complete star histories.
    </p>
    <br />
    <h3 id="Phase 3: The JavaScript to TypeScript Migration">
      Phase 3: The JavaScript to TypeScript Migration
    </h3>
    <br />
    <p>
      As the backend grew beyond basic repository listings, I hit the familiar
      problem of variables typed as <code class="code-inline">any</code> leading
      to unpredictable bugs. I was constantly using vim's
      <code class="code-inline">gd</code> command to jump into functions and
      check return types and values.
    </p>
    <br />
    <p>
      I dedicated three days to learning TypeScript essentials and switched to
      Bun as the runtime for native TypeScript support, eliminating build steps
      entirely. The migration wasn't about TypeScript evangelism; it was about
      static error checking and better developer experience.
    </p>
    <br />
    <h3 id="Phase 4: Frontend Architecture Learning">
      Phase 4: Frontend Architecture Learning
    </h3>
    <br />
    <p>
      I chose React + Vite + TailwindCSS over full-stack frameworks like
      Next.js, which felt like overkill. Despite having zero frontend
      experience, I embraced the learning opportunity.
    </p>
    <br />
    <p>
      The biggest hurdle was mastering
      <code class="code-inline">useEffect</code> for data fetching. The hook's
      complexity can lead to infinite loops and unnecessary renders. React's
      documentation on "You Might Not Need an Effect" became essential reading
      during this phase. It is easier to create a infinite loop.
    </p>
    <br />
    <p>
      I also moved away from Shadcn UI, which made the app look too generic.
      Instead, I developed a custom ASCII design language that reflects the
      developer-focused nature of the project.
    </p>
    <br />
    <h2 id="Key Architecture Decisions">Key Architecture Decisions</h2>
    <br />
    <h3 id="JIT Abstraction: Avoiding the Over-Engineering Trap">
      JIT Abstraction: Avoiding the Over-Engineering Trap
    </h3>
    <br />
    <p>
      I developed what I call "JIT abstraction," borrowing from compiler
      terminology, which means adding abstraction layers exactly when needed,
      not before.
    </p>
    <br />
    <p>The process involves:</p>
    <ol start="1" style="--data-list-start: 1">
      <li>Writing similar code patterns (2-3 similar controllers)</li>
      <li>Identifying common behaviors and pain points</li>
      <li>
        Abstracting when patterns become clear and maintenance becomes difficult
      </li>
      <li>Refactoring before complexity becomes unmanageable</li>
    </ol>
    <br />
    <p>Here's an example of controller evolution:</p>
    <br />
    <p><b>Before</b>: Fat controllers handling everything</p>
    <pre
      class="fenced-code"
    ><code class='fenced-code-content' lang='javascript'>app.get(<span class='code_string'>'/trending/:language'</span>, <span class='code_keyword'>async</span> (req, res) <span class='code_entity'>=&gt;</span> {
  <span class='code_comment'>// 130+ lines of validation, DB queries, formatting, error handling...</span>
});
</code></pre>
    <br />
    <p><b>After</b>: Lean controllers with service layer</p>
    <pre
      class="fenced-code"
    ><code class='fenced-code-content' lang='typescript'><span class='code_keyword'>export</span> <span class='code_keyword'>async</span> <span class='code_keyword'>function</span> <span class='code_function'>getTrending</span>(req<span class='code_entity'>:</span> Request, res<span class='code_entity'>:</span> Response, _next<span class='code_entity'>:</span> NextFunction)<span class='code_entity'>:</span> <span class='code_keyword'>Promise</span><span class='code_entity'>&lt;</span><span class='code_keyword'>void</span><span class='code_entity'>&gt;</span> {
  <span class='code_keyword'>try</span> {
    <span class='code_keyword'>const</span> params <span class='code_entity'>=</span> <span class='code_function'>parseTrendingParams</span>(req);
    <span class='code_keyword'>const</span> { repoList, fromCache } <span class='code_entity'>=</span> <span class='code_keyword'>await</span> <span class='code_function'>fetchTrendingWithCache</span>(params.date);
    <span class='code_keyword'>const</span> response <span class='code_entity'>=</span> <span class='code_function'>formatTrendingResponse</span>(repoList, params, fromCache);
    
    res.<span class='code_function'>status</span>(<span class='code_number'>200</span>).<span class='code_function'>json</span>(response);
  } <span class='code_keyword'>catch</span> (error) {
    <span class='code_keyword'>const</span> message <span class='code_entity'>=</span> error <span class='code_keyword'>instanceof</span> <span class='code_function'>Error</span> <span class='code_entity'>?</span> error.message <span class='code_entity'>:</span> <span class='code_string'>"Failed to fetch trending repos"</span>;
    res.<span class='code_function'>status</span>(<span class='code_number'>400</span>).<span class='code_function'>json</span>(<span class='code_function'>makeError</span>(<span class='code_keyword'>new</span> <span class='code_function'>Date</span>().<span class='code_function'>toISOString</span>(), <span class='code_number'>400</span>, message));
  }
}
</code></pre>
    <br />
    <p>
      This approach prevented both code duplication and premature optimization,
      a common trap in personal projects where you want to showcase architecture
      skills.
    </p>
    <br />
    <h3 id="Conditional Caching Strategy">Conditional Caching Strategy</h3>
    <br />
    <p>
      I implemented a <code class="code-inline">withCache</code> utility that
      combines caching logic with conditional storage:
    </p>
    <br />
    <pre
      class="fenced-code"
    ><code class='fenced-code-content' lang='typescript'><span class='code_keyword'>export</span> <span class='code_keyword'>async</span> <span class='code_keyword'>function</span> <span class='code_function'>withCache</span><span class='code_entity'>&lt;</span><span class='code_constant'>T</span><span class='code_entity'>&gt;</span>(
  cacheKey<span class='code_entity'>:</span> <span class='code_keyword'>string</span>,
  <span class='code_function'>fetchFn</span><span class='code_entity'>:</span> () <span class='code_entity'>=&gt;</span> <span class='code_keyword'>Promise</span><span class='code_entity'>&lt;</span><span class='code_constant'>T</span><span class='code_entity'>&gt;</span>,
  ttl<span class='code_entity'>:</span> <span class='code_keyword'>number</span>,
  shouldCache<span class='code_entity'>?:</span> (data<span class='code_entity'>:</span> <span class='code_constant'>T</span>) <span class='code_entity'>=&gt;</span> <span class='code_keyword'>boolean</span>,
)<span class='code_entity'>:</span> <span class='code_keyword'>Promise</span><span class='code_entity'>&lt;</span>{ data<span class='code_entity'>:</span> <span class='code_constant'>T</span>; fromCache<span class='code_entity'>:</span> <span class='code_keyword'>boolean</span> }<span class='code_entity'>&gt;</span> {
  <span class='code_keyword'>const</span> cached <span class='code_entity'>=</span> <span class='code_function'>getCache</span>(cacheKey) <span class='code_keyword'>as</span> <span class='code_constant'>T</span>;
  <span class='code_keyword'>if</span> (cached) {
    <span class='code_keyword'>return</span> { data<span class='code_entity'>:</span> cached, fromCache<span class='code_entity'>:</span> <span class='code_number'>true</span> };
  }

  <span class='code_keyword'>const</span> data <span class='code_entity'>=</span> <span class='code_keyword'>await</span> <span class='code_function'>fetchFn</span>();

  <span class='code_comment'>// Only cache if shouldCache function returns true</span>
  <span class='code_keyword'>if</span> (<span class='code_entity'>!</span>shouldCache <span class='code_entity'>||</span> <span class='code_function'>shouldCache</span>(data)) {
    <span class='code_function'>setCache</span>(cacheKey, data, ttl);
  }

  <span class='code_keyword'>return</span> { data, fromCache<span class='code_entity'>:</span> <span class='code_number'>false</span> };
}
</code></pre>
    <br />
    <p>
      This pattern allowed me to cache search results only when data was found,
      avoiding expensive cache misses for failed searches. Working within
      MongoDB's free tier constraints, every optimization mattered.
    </p>
    <br />
    <h3 id="Batched Database Operations">Batched Database Operations</h3>
    <br />
    <p>
      As the system scaled from 15 daily repositories to 150+, I moved from
      individual database calls to batched operations. This reduced database
      calls significantly while working within free tier limitations, though the
      performance improvement was less dramatic than the rate limiting
      optimizations.
    </p>
    <br />
    <h2 id="Performance & Scaling Lessons">Performance & Scaling Lessons</h2>
    <br />
    <h3 id="Rate Limiting Optimization">Rate Limiting Optimization</h3>
    <br />
    <p>
      The biggest performance breakthrough came from understanding GitHub's
      undocumented sub-rate limits. Rather than implementing a complex event
      loop system (which would still hit the 5,000/hour ceiling), I focused on
      minimizing delays and handling 403 errors intelligently.
    </p>
    <br />
    <p>
      The system now makes a single request to GitHub's rate limit endpoint when
      it encounters a 403, calculates the exact wait time, and pauses scraping
      until the limit resets. This eliminated the cascading 403 errors that
      previously extended scraping times.
    </p>
    <br />
    <h3 id="Deployment Platform Juggling">Deployment Platform Juggling</h3>
    <br />
    <p>
      Working with free tiers meant constant deployment platform switching:
      Vercel for frontend, Railway for backend (after Render's custom domain
      issues), and GitHub Actions for scheduled scraping. Each platform had its
      quirks: Bun deployment issues on Railway, MongoDB connection problems in
      GitHub Actions, CORS configuration headaches.
    </p>
    <br />
    <p>
      These constraints taught me to design systems that can adapt to different
      deployment environments rather than being tightly coupled to specific
      platforms.
    </p>
    <br />
    <h2 id="Surprising Data Insights">Surprising Data Insights</h2>
    <br />
    <p>
      The data revealed patterns that contradicted my assumptions about modern
      development:
    </p>
    <br />
    <p>
      <b>Language Distribution</b>: C and C++ still dominate trending
      repositories (32% and 19% respectively), followed by Java (17%). This was
      genuinely surprising in 2025.
    </p>
    <br />
    <p><b>Language-Specific Patterns</b>:</p>
    <ul>
      <li>
        <b>Go</b>: Primarily backend infrastructure (Kubernetes, microservices,
        databases)
      </li>
      <li>
        <b>TypeScript</b>: Heavily React-focused rather than general web
        development
      </li>
      <li>
        <b>Java</b>: Still relevant for Android, Kafka, and surprisingly,
        algorithm/interview prep
      </li>
      <li><b>Python</b>: Concentrated in AI/ML research as expected</li>
      <li>
        <b>Rust</b>: Most evenly distributed across topics, unique in blockchain
        representation
      </li>
    </ul>
    <br />
    <p>
      <b>NOTE</b>: However, I suspect this language dominance reflects
      structural differences rather than pure popularity. C/C++ projects often
      need to rebuild foundational tools that higher-level languages import as
      packages. Java's verbosity might mean even simple concepts require
      substantial codebases that attract more attention.
    </p>
    <br />
    <p>
      <b>Trending Anomalies</b>: Developers often trend without their
      repositories trending, suggesting GitHub's algorithms work differently for
      people versus projects, possibly based on contributions to established
      projects rather than new repository creation.
    </p>
    <br />
    <p>
      <b>Infrastructure Details</b>: Even seemingly simple features required
      hunting down community-maintained resources, like finding a comprehensive
      GitHub language-to-color mapping that someone had reverse-engineered and
      shared.
    </p>
    <br />
    <h2 id="What I_d Do Differently">What I'd Do Differently</h2>
    <br />
    <h3 id="Start with a Monorepo">Start with a Monorepo</h3>
    <br />
    <p>
      Keeping frontend and backend in separate repositories created deployment
      coordination headaches and duplicated TypeScript interfaces. A monorepo
      would have simplified shared types, development environment setup, and
      coordinated deployments.
    </p>
    <br />
    <h3 id="Design-First Approach">Design-First Approach</h3>
    <br />
    <p>
      Having gained experience, I would prioritize the user interface design
      earlier. The current ASCII design language evolved organically but would
      benefit from more systematic planning. Understanding how to present the
      data effectively should inform the backend API design.
    </p>
    <br />
    <h3 id="Unified Deployment Architecture">
      Unified Deployment Architecture
    </h3>
    <br />
    <p>
      Running the scraper, server, and frontend on the same platform would
      reduce complexity and deployment coordination issues. The current split
      across GitHub Actions, Railway, and Vercel works but adds unnecessary
      operational overhead.
    </p>
    <br />
    <h2 id="The Meta-Learning">The Meta-Learning</h2>
    <br />
    <p>
      Building DailyRepo taught me that good software development isn't just
      about code. It's about understanding problems, designing solutions, and
      iterating based on real-world constraints.
    </p>
    <br />
    <p>
      The temptation with full-stack development is to master every layer
      perfectly before moving forward. Instead, I learned to embrace
      "progressive competence," building working solutions while gradually
      deepening understanding of the underlying technologies.
    </p>
    <br />
    <p>
      <b>Most importantly</b>: The ability to research and learn efficiently
      often matters more than existing knowledge. Every constraint from GitHub's
      rate limits to free tier database restrictions became an opportunity to
      develop better engineering judgment.
    </p>
    <br />
    <p>
      The data collection continues automatically, the insights keep revealing
      new questions, and the codebase remains a living testament to the
      iterative nature of building something substantial alone. Sometimes the
      best way to understand what developers are actually working on is to
      become one of them.
    </p>
    <hr />
    <br />
    <p>
      <i
        >View the project at
        <a href="https://dailyrepo.tianpai.io">dailyrepo.tianpai.io</a> | Source
        code:
        <a href="https://github.com/tianpai/dailyRepo"
          >github.com/tianpai/dailyRepo</a
        ></i
      >
    </p>
  </div>
</body>
